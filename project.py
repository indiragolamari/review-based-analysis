# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10naF8G3WA2TjehQzEv1ih_s-7X3igynz
"""

!pip install deep_translator

!pip install streamlit

import pandas as pd
import numpy as np
import spacy
nlp=spacy.load('en_core_web_sm')
from wordcloud import wordcloud,STOPWORDS
import string
import re
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from nltk.tokenize import word_tokenize,sent_tokenize
from sklearn.model_selection import GridSearchCV,train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
from deep_translator import GoogleTranslator

import nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('vader_lexicon')

df = pd.read_csv("P543.csv",on_bad_lines='skip',encoding='latin-1')

df.head()

df.isnull().sum()

df.duplicated().any()

df.shape

df.describe()

df['len']=df['body'].apply(len)

df

df.groupby('rating').describe()

# Combine title and body into one text column
df['text'] = df['title'] + " " + df['body']

#intilaize tools
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english')) - {'not', 'no', 'nor', 'never'}
lemmatizer= WordNetLemmatizer()

#So it removes:
#Emojis (ðŸ˜Š, ðŸ¤”, etc.)
#Numbers (123)
#Special characters (!, @, #, etc.)
#Punctuation (., ?, etc.)
#Non-English characters
# Define a function to clean and preprocess text
def clean_text(text):
    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags
    text = text.translate(str.maketrans("", "", string.punctuation))
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove special characters and numbers
    text = text.lower()  # Lowercase
    tokens = word_tokenize(text)  # Properly tokenize
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return " ".join(tokens)

df['cleaned_text'] = df['text'].apply(clean_text)

# Create an empty list to store sentences and their ratings
sentence_data = []

#Iterate through the original DataFrame
for index, row in df.iterrows():
  rating = row['rating']  # Get the rating for the current document
  text = row['cleaned_text'] # Get the cleaned text for the current document
  sentences = sent_tokenize(text) # Tokenize the text into sentences
  # For each sentence in the document, append it and its rating to the list
  for sent in sentences:
    sentence_data.append({'text': sent, 'rating': rating})

#Create the DataFrame from the list of dictionaries
sent_df = pd.DataFrame(sentence_data)

sent_df

def label_sentiment(rating):
    if rating <= 2:
        return 'negative'
    elif rating == 3:
        return 'neutral'
    else:
        return 'positive'

sent_df['sentiment'] = sent_df['rating'].apply(label_sentiment)

sent_df

sns.countplot(data=sent_df, x='sentiment')
plt.title("Distribution of Sentiments")
plt.show()

df['review_length'] = df['text'].apply(lambda x: len(x.split()))
sns.histplot(df['review_length'], bins=30)
plt.title("Review Length Distribution")
plt.xlabel("Number of Words")
plt.show()

from wordcloud import WordCloud

for sentiment in sent_df['sentiment'].unique():
    text = " ".join(sent_df[sent_df['sentiment'] == sentiment]['text'])
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(f'WordCloud for {sentiment} Reviews')
    plt.show()

most_common_rating = sent_df['rating'].mode()[0]
print(f"The most frequent rating given by customers is: {most_common_rating}")

# we can  can also see the frequency of each rating
rating_counts = sent_df['rating'].value_counts()
print("\nFrequency of each rating:")
rating_counts

# finding the mean values of ratings
if 'rating' in df.columns:
    mean_of_rating = df['rating'].mean()
    print(f"The mean rating given by customers is: {mean_of_rating}")
else:
    print("The DataFrame does not contain a 'rating' column. Please check the column names.")
print("\033[94m*****This ratings tells us that The eproduct is above Average*****\033[0m")

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

x_train, x_test, y_train, y_test = train_test_split(sent_df['text'], sent_df['sentiment'], test_size=0.1, random_state=42)

svm = Pipeline([
    ('tfidf_vectorizer', TfidfVectorizer(ngram_range=(1,2),stop_words=None,lowercase=True)),
    ('svm_classifier', SVC(probability=True))
])

svm.fit(x_train, y_train)

y_pred = svm.predict(x_test)
print(classification_report(y_test,y_pred))

accuracy_score(y_test,y_pred)

from sklearn.linear_model import LogisticRegression
lr = Pipeline([
    ('tfidf_vectorizer', TfidfVectorizer(ngram_range=(1,2),stop_words=None,lowercase=True)),
    ('linearregression', LogisticRegression())])

lr.fit(x_train, y_train)

y_pred = lr.predict(x_test)
print(classification_report(y_test,y_pred))

accuracy_score(y_test,y_pred)

# Save the trained pipeline
joblib.dump(lr, "lr_pipeline.pkl")

